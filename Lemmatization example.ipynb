{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyiuQFFkFyJ5"
   },
   "source": [
    "Version: 07.01.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wExlTjbiFyJ8",
    "outputId": "a4cbf0f4-a600-4911-bad7-e49f312927e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (23.3.1)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.25.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "Downloading scikit_learn-1.3.2-cp311-cp311-macosx_12_0_arm64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.0\n",
      "    Uninstalling scikit-learn-1.3.0:\n",
      "      Successfully uninstalled scikit-learn-1.3.0\n",
      "Successfully installed scikit-learn-1.3.2\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.196.0.tar.gz (916 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m916.9/916.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sagemaker) (23.1.0)\n",
      "Collecting boto3<2.0,>=1.26.131 (from sagemaker)\n",
      "  Downloading boto3-1.28.73-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting cloudpickle==2.2.1 (from sagemaker)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting google-pasta (from sagemaker)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sagemaker) (1.25.0)\n",
      "Collecting protobuf<5.0,>=3.12 (from sagemaker)\n",
      "  Downloading protobuf-4.24.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker)\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker)\n",
      "  Downloading importlib_metadata-6.8.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sagemaker) (23.1)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sagemaker) (2.0.3)\n",
      "Collecting pathos (from sagemaker)\n",
      "  Downloading pathos-0.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting schema (from sagemaker)\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sagemaker) (3.8.0)\n",
      "Collecting tblib==1.7.0 (from sagemaker)\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting botocore<1.32.0,>=1.31.73 (from boto3<2.0,>=1.26.131->sagemaker)\n",
      "  Downloading botocore-1.31.73-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.26.131->sagemaker)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3<2.0,>=1.26.131->sagemaker)\n",
      "  Downloading s3transfer-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7.0,>=1.4.0->sagemaker)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Collecting ppft>=1.7.6.7 (from pathos->sagemaker)\n",
      "  Downloading ppft-1.7.6.7-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.3.7 (from pathos->sagemaker)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pox>=0.3.3 (from pathos->sagemaker)\n",
      "  Downloading pox-0.3.3-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess>=0.70.15 (from pathos->sagemaker)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting contextlib2>=0.5.5 (from schema->sagemaker)\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from botocore<1.32.0,>=1.31.73->boto3<2.0,>=1.26.131->sagemaker) (2.0.3)\n",
      "Downloading boto3-1.28.73-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
      "Downloading protobuf-4.24.4-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.4/409.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.31.73-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
      "Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.196.0-py2.py3-none-any.whl size=1223195 sha256=16582bf1dc545cea110736fc2d445b15d855b61fd5b2c2f68359d6a4970e1141\n",
      "  Stored in directory: /Users/sarahslikkerveer/Library/Caches/pip/wheels/67/09/52/7e021644a3ef1688b5fb0e9430146f2d18a30d1e9980119d21\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: zipp, tblib, smdebug-rulesconfig, protobuf, ppft, pox, jmespath, google-pasta, dill, contextlib2, cloudpickle, schema, multiprocess, importlib-metadata, botocore, s3transfer, pathos, boto3, sagemaker\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.0.0\n",
      "    Uninstalling cloudpickle-3.0.0:\n",
      "      Successfully uninstalled cloudpickle-3.0.0\n",
      "Successfully installed boto3-1.28.73 botocore-1.31.73 cloudpickle-2.2.1 contextlib2-21.6.0 dill-0.3.7 google-pasta-0.2.0 importlib-metadata-6.8.0 jmespath-1.0.1 multiprocess-0.70.15 pathos-0.3.1 pox-0.3.3 ppft-1.7.6.7 protobuf-4.24.4 s3transfer-0.7.0 sagemaker-2.196.0 schema-0.7.5 smdebug-rulesconfig-1.0.1 tblib-1.7.0 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "#Upgrade dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6gEjdmY5FyJ-",
    "outputId": "587550fc-29e0-48e0-f8f7-e1d956c85f4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE FITNESS pacer test. IS  a multistage aerobic capacity test that progressively gets more difficult as it continues. <><<The 20 meter pacer test will begin in 30 seconds. <> ?Line up at the start.? ~ The running speed starts slowly,      but gets faster each minute after you hear this signal. [beep] A single lap should be completed each time you hear this sound. [ding] Remember to run in a straight line, and run as long as possible. The second time you fail to complete a lap before the sound, your test is over.... , --- \n"
     ]
    }
   ],
   "source": [
    "text = \"THE FITNESS pacer test. IS  a multistage aerobic capacity test that progressively gets more difficult as it continues. <><<The 20 meter pacer test will begin in 30 seconds. <> ?Line up at the start.? ~ The running speed starts slowly,      but gets faster each minute after you hear this signal. [beep] A single lap should be completed each time you hear this sound. [ding] Remember to run in a straight line, and run as long as possible. The second time you fail to complete a lap before the sound, your test is over.... , --- \"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4wlH5aKFyJ_"
   },
   "source": [
    "First, change the text so that it's all lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "LmRQprFyFyJ_",
    "outputId": "50124379-2d31-4989-9edb-0ab1e0d5be59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fitness pacer test. is  a multistage aerobic capacity test that progressively gets more difficult as it continues. <><<the 20 meter pacer test will begin in 30 seconds. <> ?line up at the start.? ~ the running speed starts slowly,      but gets faster each minute after you hear this signal. [beep] a single lap should be completed each time you hear this sound. [ding] remember to run in a straight line, and run as long as possible. the second time you fail to complete a lap before the sound, your test is over.... , --- \n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yy-8DKQNFyJ_"
   },
   "source": [
    "Next, remove leading whitespace or trailing whitespace with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "iwoP3TIaFyKA",
    "outputId": "ba4a56a7-af3b-43bd-9309-e967d32b0e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fitness pacer test. is  a multistage aerobic capacity test that progressively gets more difficult as it continues. <><<the 20 meter pacer test will begin in 30 seconds. <> ?line up at the start.? ~ the running speed starts slowly,      but gets faster each minute after you hear this signal. [beep] a single lap should be completed each time you hear this sound. [ding] remember to run in a straight line, and run as long as possible. the second time you fail to complete a lap before the sound, your test is over.... , ---\n"
     ]
    }
   ],
   "source": [
    "text = text.strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1jHOMugFyKA"
   },
   "source": [
    "Use a regular expression to remove HTML tags or markup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8LutPRcWFyKB",
    "outputId": "31cdcea3-cebc-4fc5-a199-b8c2c53411f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fitness pacer test. is  a multistage aerobic capacity test that progressively gets more difficult as it continues.  ?line up at the start.? ~ the running speed starts slowly,      but gets faster each minute after you hear this signal. [beep] a single lap should be completed each time you hear this sound. [ding] remember to run in a straight line, and run as long as possible. the second time you fail to complete a lap before the sound, your test is over.... , ---\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = re.compile('<.*?>').sub('', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDNJXrtnFyKB"
   },
   "source": [
    "Replace punctuation with a space. Be careful with this task. Depending on the application, punctuation can actually be useful. For example, punctuation might affect the positive or negative meaning of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "C2mbcCQIFyKB",
    "outputId": "5fdf8c4c-cfaf-4e6a-97e3-fc8eec92f2a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fitness pacer test  is  a multistage aerobic capacity test that progressively gets more difficult as it continues    line up at the start     the running speed starts slowly       but gets faster each minute after you hear this signal   beep  a single lap should be completed each time you hear this sound   ding  remember to run in a straight line  and run as long as possible  the second time you fail to complete a lap before the sound  your test is over          \n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ6Df3NXFyKB"
   },
   "source": [
    "Remove any extra spaces and tabs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "gDt81VV8FyKC",
    "outputId": "af1f9ab0-90b9-40f8-ad32-581a6fe6c92b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fitness pacer test is a multistage aerobic capacity test that progressively gets more difficult as it continues line up at the start the running speed starts slowly but gets faster each minute after you hear this signal beep a single lap should be completed each time you hear this sound ding remember to run in a straight line and run as long as possible the second time you fail to complete a lap before the sound your test is over \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = re.sub('\\s+', ' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XGA5xNG-FyKC",
    "outputId": "535229e5-bab7-40ce-8a0a-7ca175a7be14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sarahslikkerveer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sarahslikkerveer/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sarahslikkerveer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3mjmoSfFyKD"
   },
   "source": [
    "#### Stopword removal\n",
    "Some words in sentences can occur very frequently, and they don't contribute too much to the overall meaning of the sentences. Typically, you would use list of these words and remove them from each sentence. For example, stopwords include: *a*, *an*, *the*, *this*, *that*, *is*, *it*, *to*, and *and*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "BeyCiZg7FyKD"
   },
   "outputs": [],
   "source": [
    "# Use a tokenizer from the NLTK library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "# Stopword lists can be adjusted for your problem\n",
    "stopwords = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\", \"as\", \"in\", \"at\", \"you\", \"be\", \"your\"]\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(text)\n",
    "for w in words:\n",
    "    if w not in stopwords:\n",
    "        filtered_sentence.append(w)\n",
    "text = \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JaOaNfcyFyKD",
    "outputId": "44fb7d30-c402-4b21-80ba-7f90337ba0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitness pacer test multistage aerobic capacity test progressively gets more difficult continues line up start running speed starts slowly but gets faster each minute after hear signal beep single lap should completed each time hear sound ding remember run straight line run long possible second time fail complete lap before sound test over\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz94PuIPFyKE"
   },
   "source": [
    "#### Stemming words\n",
    "Stemming is a rule-based system for converting words into their root form. It removes suffixes from words. This process helps enhance similarities (if any) between sentences. \n",
    "\n",
    "Examples:\n",
    "\n",
    "\"jumping\", \"jumped\" -> \"jump\"\n",
    "\n",
    "\"cars\" -> \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lMDjX9PrFyKE"
   },
   "outputs": [],
   "source": [
    "# Use a tokenizer and stemmer from the NLTK library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "stemmed_sentence = []\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(text)\n",
    "for w in words:\n",
    "    # Stem the word/token\n",
    "    stemmed_sentence.append(snow.stem(w))\n",
    "stemmed_text = \" \".join(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "TXj9Ix8OFyKE",
    "outputId": "d1b0f419-c5ee-4fa4-8dd1-1d5e746f7e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit pacer test multistag aerob capac test progress get more difficult continu line up start run speed start slowli but get faster each minut after hear signal beep singl lap should complet each time hear sound ding rememb run straight line run long possibl second time fail complet lap befor sound test over\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPEwXkqSFyKE"
   },
   "source": [
    "From the previous cell, you can see that the stemming operation is *not* perfect. It generated some mistakes, such as *messag*, *involv*, and *adjac*. Stemming is a rule-based method that sometimes mistakenly remove suffixes from words. Nevertheless, it runs quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD7aeQklFyKF"
   },
   "source": [
    "#### Lemmatizing words\n",
    "If you are not satisfied with the result of stemming, you can use lemmatization instead. It usually requires more work, but it gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5zba2FgOFyKF",
    "outputId": "21fb61b7-8137-4105-a9ad-5bcfbccc405a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sarahslikkerveer/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary functions\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "lemmatized_sentence = []\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(text)\n",
    "# Get position tags\n",
    "word_pos_tags = nltk.pos_tag(words)\n",
    "# Map the position tag and lemmatize the word or token\n",
    "for idx, tag in enumerate(word_pos_tags):\n",
    "    lemmatized_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n",
    "\n",
    "lemmatized_text = \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "j6qhyfBJFyKF",
    "outputId": "1ff7ff89-c0ff-46a9-b5f2-f39c93de4d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitness pacer test multistage aerobic capacity test progressively get more difficult continue line up start run speed start slowly but get fast each minute after hear signal beep single lap should complete each time hear sound ding remember run straight line run long possible second time fail complete lap before sound test over\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3tMrX-0FyKF"
   },
   "source": [
    "You can use the tasks you completed in this notebook for many of the business problems that you will work on in this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaZRvCxAFyKF"
   },
   "source": [
    "*©2021 Amazon Web Services, Inc. or its affiliates. All rights reserved. This work may not be reproduced or redistributed, in whole or in part, without prior written permission from Amazon Web Services, Inc. Commercial copying, lending, or selling is prohibited. All trademarks are the property of their owners.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
